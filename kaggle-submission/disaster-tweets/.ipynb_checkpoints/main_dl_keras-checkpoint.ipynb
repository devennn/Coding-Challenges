{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "import string\n",
    "import pickle as pkl\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import optimizers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('.').parent.absolute()\n",
    "\n",
    "full_train = os.path.join(path, 'raw-dataset', 'train.csv')\n",
    "train_df = pd.read_csv(full_train, encoding='utf-8')\n",
    "\n",
    "full_test = os.path.join(path, 'raw-dataset', 'test.csv')\n",
    "test_df = pd.read_csv(full_test, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(tweet_text, df):\n",
    "    temp = []\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    for tweet in tweet_text:\n",
    "        # Remove links\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "        # Remove newline\n",
    "        tweet = tweet.strip('\\n')\n",
    "        # Remove unicode\n",
    "        tweet = normalize('NFKD', tweet).encode('ascii','ignore')\n",
    "        # Remove username\n",
    "        tweet = re.sub('@[^\\s]+','',str(tweet))\n",
    "        # Remove punctuation and change to lower case\n",
    "        tweet = tweet.translate(table).lower()\n",
    "        # Remove 'b' at the begining for binary\n",
    "        tweet = tweet.replace('b', '', 1)\n",
    "        # Remove whitespace at start of sentence\n",
    "        tweet = tweet.strip()\n",
    "#         # Remove numbers\n",
    "#         tweet = ''.join([i for i in tweet if not i.isdigit()])\n",
    "        temp.append(tweet)\n",
    "    try:\n",
    "        # Concatenate training with target\n",
    "        processed_tweets = pd.concat([pd.DataFrame(temp), df['target']], axis=1)\n",
    "        processed_tweets = pd.DataFrame(processed_tweets)\n",
    "    except KeyError:\n",
    "        processed_tweets = pd.DataFrame(temp)\n",
    "    print(processed_tweets)\n",
    "    return processed_tweets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0  target\n",
      "0     our deeds are the reason of this earthquake ma...       1\n",
      "1                 forest fire near la ronge sask canada       1\n",
      "2     all residents asked to shelter in place are be...       1\n",
      "3     13000 people receive wildfires evacuation orde...       1\n",
      "4     just got sent this photo from ruby alaska as s...       1\n",
      "...                                                 ...     ...\n",
      "7608  two giant cranes holding a bridge collapse int...       1\n",
      "7609  the out of control wild fires in california ev...       1\n",
      "7610               m194 0104 utc5km s of volcano hawaii       1\n",
      "7611  police investigating after an ebike collided w...       1\n",
      "7612  the latest more homes razed by northern califo...       1\n",
      "\n",
      "[7613 rows x 2 columns]\n",
      "                                                      0\n",
      "0                    just happened a terrible car crash\n",
      "1     heard about earthquake is different cities sta...\n",
      "2     there is a forest fire at spot pond geese are ...\n",
      "3                 apocalypse lighting spokane wildfires\n",
      "4         typhoon soudelor kills 28 in china and taiwan\n",
      "...                                                 ...\n",
      "3258  earthquake safety los angeles uo safety fasten...\n",
      "3259  storm in ri worse than last hurricane my citya...\n",
      "3260                   green line derailment in chicago\n",
      "3261           meg issues hazardous weather outlook hwo\n",
      "3262  cityofcalgary has activated its municipal emer...\n",
      "\n",
      "[3263 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training and testing tweets\n",
    "processed_tr_tweets = cleaning(train_df['text'], train_df)\n",
    "processed_tst_tweets = cleaning(test_df['text'], test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tweets(count_vect, data):\n",
    "    vect_tweets = count_vect.fit_transform(data)\n",
    "    vect_tweets = vect_tweets.toarray()\n",
    "    return vect_tweets, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training length: 7613\n",
      "Testing length: 3263\n",
      "Length of train + test: 10876\n"
     ]
    }
   ],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "count_vect = CountVectorizer(analyzer='word', lowercase=False, stop_words='english')\n",
    "# Combine both train and test\n",
    "# Prevent unequal length of variables after tokenization\n",
    "combined_tr_tst = pd.concat([processed_tr_tweets[0], processed_tst_tweets[0]], axis=0)\n",
    "combined_vect,_ = vectorize_tweets(count_vect, combined_tr_tst)\n",
    "\n",
    "# Check length\n",
    "len_tr = len(processed_tr_tweets[0])\n",
    "print('Training length: %d' %len_tr)\n",
    "len_tst = len(processed_tst_tweets[0])\n",
    "print('Testing length: %d' %len_tst)\n",
    "print('Length of train + test: %d' %len(combined_vect))\n",
    "\n",
    "# Split back to train and test\n",
    "vect_tweets = combined_vect[:len_tr]\n",
    "vect_tst_tweets = combined_vect[len_tr:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and testing\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        vect_tweets, \n",
    "        processed_tr_tweets['target'],\n",
    "        train_size=0.80, \n",
    "        random_state=True,\n",
    "        shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(new_prediction, fname):\n",
    "    new_prediction = new_prediction.rename({0: 'target'}, axis=1) \n",
    "    new_prediction.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(input_len, output_len):\n",
    "    n_hidden_1 = math.ceil(input_len / 2)\n",
    "    n_hidden_2 = math.ceil(n_hidden_1 / 2)\n",
    "    n_hidden_3 = n_hidden_2\n",
    "    n_hidden_4 = math.ceil(input_len / 2)\n",
    "\n",
    "    Inp = Input(shape=(input_len, ))\n",
    "    x = Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\")(Inp)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\n",
    "    output = Dense(output_len, activation='softmax', name = \"Output_Layer\")(x)\n",
    "                \n",
    "    model = Model(Inp, output)\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 19847)]           0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 9924)              196971552 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9924)              0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 4962)              49247850  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4962)              0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 4962)              24626406  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4962)              0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 9924)              49252812  \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 2)                 19850     \n",
      "=================================================================\n",
      "Total params: 320,118,470\n",
      "Trainable params: 320,118,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_len = X_train.shape[1]\n",
    "model = define_model(input_len, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_train, y_test):\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.1\n",
    "    adam = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "    model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size = 100,\n",
    "            epochs = 1,\n",
    "            validation_data=(X_test, y_test),\n",
    "            shuffle=True\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "6090/6090 [==============================] - 209s 34ms/sample - loss: 0.6693 - accuracy: 0.7131 - val_loss: 0.4647 - val_accuracy: 0.7971\n"
     ]
    }
   ],
   "source": [
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "model = train(model, X_train, X_test, y_train_np, y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "...  ..\n",
      "3258  0\n",
      "3259  0\n",
      "3260  0\n",
      "3261  0\n",
      "3262  0\n",
      "\n",
      "[3263 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "dl_predictions = pd.DataFrame(model.predict(vect_tst_tweets))\n",
    "dl_rounded = pd.DataFrame([int(x) for x in dl_predictions[1]])\n",
    "print(dl_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  0\n",
      "0         0  0\n",
      "1         2  0\n",
      "2         3  0\n",
      "3         9  0\n",
      "4        11  0\n",
      "...     ... ..\n",
      "3258  10861  0\n",
      "3259  10865  0\n",
      "3260  10868  0\n",
      "3261  10874  0\n",
      "3262  10875  0\n",
      "\n",
      "[3263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "format_predictions = pd.concat([test_df['id'], dl_rounded], axis=1)\n",
    "print(format_predictions)\n",
    "\n",
    "# save_submission(format_predictions, 'submission3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
